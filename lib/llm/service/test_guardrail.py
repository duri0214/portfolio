from unittest.mock import Mock, patch, MagicMock

from django.test import TestCase

from lib.llm.service.guardrail import ModerationService, SemanticGuardService
from lib.llm.valueobject.guardrail import GuardRailSignal, SemanticGuardException


def create_mock_safe_response() -> Mock:
    """
    OpenAI Moderation API ã®ã€Œå®‰å…¨ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€ã‚’æ¨¡å€£ã—ãŸãƒ¢ãƒƒã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚

    ã“ã®é–¢æ•°ã¯ã€flagged=Falseï¼ˆå®‰å…¨ï¼‰ã‹ã¤ã™ã¹ã¦ã®ã‚«ãƒ†ã‚´ãƒªãŒ False ã®
    ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’è¿”ã™ã‚ˆã†ã«è¨­å®šã•ã‚ŒãŸãƒ¢ãƒƒã‚¯ã‚’è¿”ã—ã¾ã™ã€‚

    ä¸»ã«ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã«ãŠã„ã¦ã€æ­£å¸¸ç³»ï¼ˆsafe contentï¼‰ã®ã‚·ãƒŠãƒªã‚ªã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã—ã¾ã™ã€‚

    Returns:
        Mock: OpenAI Moderation API ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã‚’æ¨¡å€£ã—ãŸãƒ¢ãƒƒã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

    å‚è€ƒ:
        OpenAI Moderation API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹:
        https://platform.openai.com/docs/guides/moderation/overview
    """
    mock_response = Mock()
    result = Mock()
    result.flagged = False
    result.categories.model_dump.return_value = {
        "violence": False,
        "hate": False,
        "harassment": False,
        "self-harm": False,
        "sexual": False,
        "sexual/minors": False,
        "violence/graphic": False,
        "hate/threatening": False,
        "harassment/threatening": False,
        "self-harm/intent": False,
        "self-harm/instructions": False,
    }
    mock_response.results = [result]
    return mock_response


def create_mock_unsafe_response() -> Mock:
    """
    OpenAI Moderation API ã®ã€Œä¸é©åˆ‡ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆflagged=Trueï¼‰ã€ã‚’æ¨¡å€£ã—ãŸãƒ¢ãƒƒã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚

    ã“ã®é–¢æ•°ã¯ã€flagged=Trueï¼ˆå±é™ºï¼‰ã‹ã¤ "violence" ã‚«ãƒ†ã‚´ãƒªã®ã¿ True ã®
    ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’è¿”ã™ã‚ˆã†ã«è¨­å®šã•ã‚ŒãŸãƒ¢ãƒƒã‚¯ã‚’è¿”ã—ã¾ã™ã€‚

    ä¸»ã«ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã«ãŠã„ã¦ã€ç•°å¸¸ç³»ï¼ˆunsafe contentï¼‰ã®ã‚·ãƒŠãƒªã‚ªã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã—ã¾ã™ã€‚

    Returns:
        Mock: OpenAI Moderation API ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã‚’æ¨¡å€£ã—ãŸãƒ¢ãƒƒã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

    å‚è€ƒ:
        OpenAI Moderation API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹:
        https://platform.openai.com/docs/guides/moderation/overview
    """
    mock_response = Mock()
    result = Mock()
    result.flagged = True
    result.categories.model_dump.return_value = {
        "violence": True,
        "hate": False,
        "harassment": False,
        "self-harm": False,
        "sexual": False,
        "sexual/minors": False,
        "violence/graphic": False,
        "hate/threatening": False,
        "harassment/threatening": False,
        "self-harm/intent": False,
        "self-harm/instructions": False,
    }
    mock_response.results = [result]
    return mock_response


class TestModerationService(TestCase):
    """
    ModerationService ã‚¯ãƒ©ã‚¹ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã€‚

    æœ¬ã‚¯ãƒ©ã‚¹ã§ã¯ã€OpenAI Moderation API ã®ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”¨ã„ã¦ã€
    ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å®‰å…¨æ€§è©•ä¾¡ã«é–¢ã™ã‚‹å„ç¨®æ©Ÿèƒ½ã‚’æ¤œè¨¼ã™ã‚‹ã€‚

    ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ãƒ¡ã‚½ãƒƒãƒ‰:
        - check_input_moderation: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
        - check_output_moderation: AIå‡ºåŠ›ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
        - create_guardrail: å…¥åŠ›ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é–¢æ•°ã®ç”Ÿæˆ
        - create_output_moderation_guardrail: å‡ºåŠ›ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é–¢æ•°ã®ç”Ÿæˆ

    ä¸»ãªæ¤œè¨¼è¦³ç‚¹:
        1. æ­£å¸¸ç³»ãƒ†ã‚¹ãƒˆ:
            - å®‰å…¨ãªãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ blocked=False ã‚’è¿”ã™ã‹
            - categories ãŒç©ºã§ã‚ã‚‹ã‹
        2. ç•°å¸¸ç³»ãƒ†ã‚¹ãƒˆ:
            - å±é™ºãªãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ blocked=True ã¨é©åˆ‡ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ»ã‚«ãƒ†ã‚´ãƒªã‚’è¿”ã™ã‹
        3. ã‚¨ãƒ©ãƒ¼ç³»ãƒ†ã‚¹ãƒˆ:
            - OpenAI APIãŒä¾‹å¤–ã‚’è¿”ã—ãŸéš›ã«ã€éå³æ ¼ãƒ¢ãƒ¼ãƒ‰ã§ã¯è¨±å¯ã•ã‚Œã‚‹ã‹
            - å³æ ¼ãƒ¢ãƒ¼ãƒ‰ã§ã¯å®‰å…¨å´ï¼ˆblocked=Trueï¼‰ã«å€’ã‚Œã‚‹ã‹
        4. ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é–¢æ•°ã®å‹•ä½œæ¤œè¨¼:
            - callable ã§ã‚ã‚‹ã“ã¨
            - å†…éƒ¨ã§é©åˆ‡ã« ModerationService ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã“ã¨

    ãƒ†ã‚¹ãƒˆè¨­è¨ˆä¸Šã®ç‰¹å¾´:
        - OpenAI Moderation API ã®ä»•æ§˜ã«å¾“ã£ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒƒã‚¯ã‚’é–¢æ•°ã§å…±é€šåŒ–
        - ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã¯ VO (SemanticGuardResult) ã§è¡¨ç¾

    å‚è€ƒ: OpenAI Moderation API ä»•æ§˜
    https://platform.openai.com/docs/guides/moderation
    """

    @patch("lib.llm.service.guardrail.OpenAI")
    def setUp(self, mock_openai):
        # OpenAI APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’ãƒ¢ãƒƒã‚¯åŒ–
        self.mock_client = Mock()
        mock_openai.return_value = self.mock_client

        self.service = ModerationService()
        self.entity_name = "ãƒ†ã‚¹ãƒˆã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£"
        self.safe_text = "ã“ã‚“ã«ã¡ã¯ã€ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿ"
        self.unsafe_text = "æš´åŠ›çš„ãªå†…å®¹ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆ"

        self.mock_safe_response = create_mock_safe_response()
        self.mock_unsafe_response = create_mock_unsafe_response()

    def test_check_input_moderation_safe_content(self):
        """
        [æ­£å¸¸ç³»] ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãŒå®‰å…¨ãªå ´åˆã®ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å‹•ä½œã‚’æ¤œè¨¼ã™ã‚‹

        ã‚·ãƒŠãƒªã‚ª:
            - OpenAI Moderation API ã«å®‰å…¨ãªãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã™
            - APIã¯ `flagged=False`ï¼ˆé•åãªã—ï¼‰ã‚’è¿”ã™
            - ã‚µãƒ¼ãƒ“ã‚¹ã¯ `blocked=False`ã€ã‹ã¤ `message=""`ï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã—ï¼‰ã‚’è¿”ã™

        æœŸå¾…çµæœ:
            - `blocked` ã¯ Falseï¼ˆå‡¦ç†ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œãªã„ï¼‰
            - `message` ã¯ç©ºæ–‡å­—åˆ—ï¼ˆã‚¨ãƒ©ãƒ¼ãªã—ï¼å®‰å…¨ãªçŠ¶æ…‹ï¼‰
            - `categories` ã«é•åã‚«ãƒ†ã‚´ãƒªãŒå«ã¾ã‚Œãªã„

        é‡è¦åº¦:
            é«˜ â€” é€šå¸¸åˆ©ç”¨æ™‚ã®æƒ³å®šãƒ‘ã‚¹ã§èª¤ãƒ–ãƒ­ãƒƒã‚¯ãŒãªã„ã“ã¨ã‚’ç¢ºèª
        """
        # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½¿ã£ã¦çµ±ä¸€
        self.mock_client.moderations.create.return_value = self.mock_safe_response

        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        result = self.service.check_input_moderation(
            self.safe_text, self.entity_name, strict_mode=False
        )

        # çµæœæ¤œè¨¼
        self.assertEqual(result.signal, GuardRailSignal.GREEN)
        self.assertEqual(result.detail, None)

        # APIå‘¼ã³å‡ºã—ã®æ¤œè¨¼
        self.mock_client.moderations.create.assert_called_once_with(
            model="omni-moderation-latest", input=self.safe_text
        )

    def test_check_input_moderation_unsafe_content(self):
        """
        [ç•°å¸¸ç³»] ä¸é©åˆ‡ãªãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¯¾ã™ã‚‹æ¤œå‡ºå‡¦ç†ã‚’æ¤œè¨¼ã™ã‚‹

        ã‚·ãƒŠãƒªã‚ª:
            - ä¸é©åˆ‡ãªãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›
            - APIã¯ `flagged=True`ã€ã‹ã¤é•åã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹: violenceï¼‰ã‚’è¿”ã™
            - ã‚µãƒ¼ãƒ“ã‚¹ã¯ `blocked=True`ã€ç†ç”±ã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€é•åã‚«ãƒ†ã‚´ãƒªã‚’è¿”ã™

        æœŸå¾…çµæœ:
            - `blocked` ã¯ Trueï¼ˆå‡¦ç†ã‚’ãƒ–ãƒ­ãƒƒã‚¯ï¼‰
            - `message` ã«ãƒ–ãƒ­ãƒƒã‚¯ç†ç”±ã¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã‚’å«ã‚€
            - `categories` ã«è©²å½“ã™ã‚‹é•åã‚«ãƒ†ã‚´ãƒªãŒå«ã¾ã‚Œã‚‹

        é‡è¦åº¦:
            é«˜ â€” ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¿è­·ãƒ»è¦ç´„éµå®ˆã®ãŸã‚ã€ç¢ºå®Ÿãªæ¤œå‡ºãŒå¿…é ˆ
        """
        # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½¿ã£ã¦çµ±ä¸€
        self.mock_client.moderations.create.return_value = self.mock_unsafe_response

        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        result = self.service.check_input_moderation(
            self.unsafe_text, self.entity_name, strict_mode=False
        )

        # çµæœæ¤œè¨¼
        self.assertEqual(result.signal, GuardRailSignal.RED)
        self.assertIn(
            "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ãã®å†…å®¹ã¯é©åˆ‡ã§ã¯ãªã„ãŸã‚ã€ãŠç­”ãˆã§ãã¾ã›ã‚“",
            result.detail,
        )
        self.assertIn(self.entity_name, result.detail)
        self.assertIn("violence", result.detail)

    def test_check_input_moderation_api_error_non_strict(self):
        """
        [ã‚¨ãƒ©ãƒ¼ç³»] APIä¾‹å¤–ç™ºç”Ÿæ™‚ã€strict_mode=Falseã§ã®å¯›å®¹ãªå‡¦ç†ã‚’æ¤œè¨¼ã™ã‚‹

        ã‚·ãƒŠãƒªã‚ª:
            - Moderation API ãŒä¾‹å¤–ã‚’ç™ºç”Ÿ
            - strict_mode=False ã§å‘¼ã³å‡ºã™
            - ã‚µãƒ¼ãƒ“ã‚¹ã¯ãƒ­ã‚°ã«è­¦å‘Šã‚’å‡ºã—ã¤ã¤ `blocked=False` ã§å‡¦ç†ã‚’è¨±å¯

        æœŸå¾…çµæœ:
            - `blocked` ã¯ Falseï¼ˆå‡¦ç†ã¯è¨±å¯ï¼‰
            - `message` ã¯ç©ºæ–‡å­—åˆ—ï¼ˆã‚¨ãƒ©ãƒ¼è¡¨ç¤ºãªã—ï¼‰
            - ãƒ­ã‚°ã«ã‚¨ãƒ©ãƒ¼æƒ…å ±ãŒå‡ºåŠ›ã•ã‚Œã‚‹

        é‡è¦åº¦:
            ä¸­ â€” ä¸€æ™‚çš„APIéšœå®³ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã‚’éåº¦ã«æ‚ªåŒ–ã•ã›ãªã„è¨­è¨ˆ
        """
        # ãƒ¢ãƒƒã‚¯ã§APIä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹
        self.mock_client.moderations.create.side_effect = Exception("API Error")

        # ãƒ­ã‚°ã®ãƒ¢ãƒƒã‚¯
        with patch("lib.llm.service.guardrail.logger") as mock_logger:
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            result = self.service.check_input_moderation(
                self.safe_text, self.entity_name, strict_mode=False
            )

            # çµæœæ¤œè¨¼
            self.assertEqual(result.signal, GuardRailSignal.GREEN)
            self.assertIsNone(result.detail)

            # ãƒ­ã‚°å‡ºåŠ›ã®æ¤œè¨¼
            mock_logger.warning.assert_called_once()
            self.assertIn(
                "OpenAI Moderation API error", mock_logger.warning.call_args[0][0]
            )

    def test_check_input_moderation_api_error_strict(self):
        """
        [ã‚¨ãƒ©ãƒ¼ç³»] APIä¾‹å¤–ç™ºç”Ÿæ™‚ã€strict_mode=Trueã§å®‰å…¨å´ã«å€’ã™å‡¦ç†ã‚’æ¤œè¨¼ã™ã‚‹

        ã‚·ãƒŠãƒªã‚ª:
            - Moderation API ãŒä¾‹å¤–ã‚’ç™ºç”Ÿ
            - strict_mode=True ã§å‘¼ã³å‡ºã™
            - ã‚µãƒ¼ãƒ“ã‚¹ã¯ `blocked=True`ã€å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ä¸èƒ½ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™

        æœŸå¾…çµæœ:
            - `blocked` ã¯ Trueï¼ˆå‡¦ç†ã‚’ãƒ–ãƒ­ãƒƒã‚¯ï¼‰
            - `message` ã«å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ä¸å¯ã®æ—¨ã¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã‚’å«ã‚€

        é‡è¦åº¦:
            é«˜ â€” å®‰å…¨æœ€å„ªå…ˆã®é‹ç”¨ãƒ¢ãƒ¼ãƒ‰ã§ãƒªã‚¹ã‚¯æ’é™¤ã‚’ä¿è¨¼
        """
        # ãƒ¢ãƒƒã‚¯ã§APIä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹
        self.mock_client.moderations.create.side_effect = Exception("API Error")

        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        result = self.service.check_input_moderation(
            self.safe_text, self.entity_name, strict_mode=True
        )

        # çµæœæ¤œè¨¼
        self.assertEqual(result.signal, GuardRailSignal.RED)
        self.assertIn("ç¾åœ¨ã€å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“", result.detail)
        self.assertIn(self.entity_name, result.detail)

    def test_check_output_moderation_safe_content(self):
        """
        [æ­£å¸¸ç³»] AIå‡ºåŠ›ãŒå®‰å…¨ãªå ´åˆã®ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†ã‚’æ¤œè¨¼ã™ã‚‹

        ã‚·ãƒŠãƒªã‚ª:
            - å®‰å…¨ãªå‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›
            - APIã¯ `flagged=False` ã‚’è¿”ã™
            - ã‚µãƒ¼ãƒ“ã‚¹ã¯ `blocked=False`ã€ç©ºãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™

        æœŸå¾…çµæœ:
            - `blocked` ã¯ Falseï¼ˆå‡¦ç†ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œãªã„ï¼‰
            - `message` ã¯ç©ºæ–‡å­—åˆ—ï¼ˆã‚¨ãƒ©ãƒ¼ãªã—ï¼‰

        é‡è¦åº¦:
            é«˜ â€” é€šå¸¸ã®å¿œç­”ãŒæ­£å¸¸ã«å‡¦ç†ã•ã‚Œã‚‹ã“ã¨ã‚’ä¿è¨¼
        """
        # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½¿ã£ã¦çµ±ä¸€
        self.mock_client.moderations.create.return_value = self.mock_safe_response

        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        result = self.service.check_output_moderation(self.safe_text, self.entity_name)

        # çµæœæ¤œè¨¼
        self.assertEqual(result.signal, GuardRailSignal.GREEN)
        self.assertIsNone(result.detail)

    def test_check_output_moderation_unsafe_content(self):
        """
        [ç•°å¸¸ç³»] AIå‡ºåŠ›ãŒä¸é©åˆ‡ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å ´åˆã®æ¤œå‡ºã‚’æ¤œè¨¼ã™ã‚‹

        ã‚·ãƒŠãƒªã‚ª:
            - å±é™ºãªå‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›
            - APIã¯ `flagged=True` ã‚’è¿”ã™
            - ã‚µãƒ¼ãƒ“ã‚¹ã¯ `blocked=True`ã€è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™

        æœŸå¾…çµæœ:
            - `blocked` ã¯ Trueï¼ˆå‡ºåŠ›ã‚’ãƒ–ãƒ­ãƒƒã‚¯ï¼‰
            - `message` ã«è­¦å‘Šæ–‡è¨€ã¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã‚’å«ã‚€

        é‡è¦åº¦:
            é«˜ â€” ä¸é©åˆ‡ãªå¿œç­”é˜²æ­¢ã®ãŸã‚å¿…é ˆ
        """
        # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½¿ã£ã¦çµ±ä¸€
        self.mock_client.moderations.create.return_value = self.mock_unsafe_response

        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        result = self.service.check_output_moderation(
            self.unsafe_text, self.entity_name
        )

        # çµæœæ¤œè¨¼
        self.assertEqual(result.signal, GuardRailSignal.RED)
        self.assertIn("é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ", result.detail)
        self.assertIn(self.entity_name, result.detail)

    def test_create_guardrail(self):
        """
        å…¥åŠ›ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é–¢æ•°ã®ç”Ÿæˆã¨å‹•ä½œç¢ºèªã‚’è¡Œã†

        ã‚·ãƒŠãƒªã‚ª:
            - `create_guardrail` ã§é–¢æ•°ç”Ÿæˆ
            - ç”Ÿæˆé–¢æ•°ãŒãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å‘¼ã³å‡ºã—ã€æ­£ã—ããƒ–ãƒ­ãƒƒã‚¯åˆ¤å®šã‚’è¿”ã™ã‹æ¤œè¨¼

        æœŸå¾…çµæœ:
            - å‘¼ã³å‡ºã—å¯èƒ½ãªé–¢æ•°ãŒè¿”ã‚‹
            - ä¸é©åˆ‡ãƒ†ã‚­ã‚¹ãƒˆã§ `blocked=True` ã‚’è¿”ã™

        é‡è¦åº¦:
            ä¸­ â€” Agents SDK é€£æºã®æ¥ç¶šç‚¹ã¨ã—ã¦æ­£ç¢ºãªå‹•ä½œãŒå¿…è¦
        """
        # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½¿ã£ã¦çµ±ä¸€
        self.mock_client.moderations.create.return_value = self.mock_unsafe_response

        # ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é–¢æ•°ã®ç”Ÿæˆ
        guardrail_func = self.service.create_guardrail(
            self.entity_name, strict_mode=True
        )

        # é–¢æ•°ãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        self.assertTrue(callable(guardrail_func))

        # ç”Ÿæˆã•ã‚ŒãŸé–¢æ•°ã®å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
        result = guardrail_func(None, None, self.safe_text)

        # çµæœæ¤œè¨¼
        self.assertTrue(result["blocked"])

    def test_create_output_moderation_guardrail(self):
        """
        å‡ºåŠ›ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é–¢æ•°ã®ç”Ÿæˆã¨å‹•ä½œæ¤œè¨¼ã‚’è¡Œã†

        ã‚·ãƒŠãƒªã‚ª:
            - `create_output_moderation_guardrail` ã§é–¢æ•°ç”Ÿæˆ
            - ç”Ÿæˆé–¢æ•°ãŒå‡ºåŠ›ã®ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„é©åˆ‡ã«ãƒ–ãƒ­ãƒƒã‚¯åˆ¤å®šã™ã‚‹ã‹æ¤œè¨¼

        æœŸå¾…çµæœ:
            - å‘¼ã³å‡ºã—å¯èƒ½ãªé–¢æ•°ãŒè¿”ã‚‹
            - ä¸é©åˆ‡å‡ºåŠ›ã§ `blocked=True` ã‚’è¿”ã™

        é‡è¦åº¦:
            ä¸­ â€” å¿œç­”å†…å®¹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿæ§‹ã¨ã—ã¦å¿…é ˆ
        """
        # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½¿ã£ã¦çµ±ä¸€
        self.mock_client.moderations.create.return_value = self.mock_unsafe_response

        # ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é–¢æ•°ã®ç”Ÿæˆ
        guardrail_func = self.service.create_output_moderation_guardrail(
            self.entity_name
        )

        # é–¢æ•°ãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        self.assertTrue(callable(guardrail_func))

        # ç”Ÿæˆã•ã‚ŒãŸé–¢æ•°ã®å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
        result = guardrail_func(None, None, self.unsafe_text)

        # çµæœæ¤œè¨¼
        self.assertTrue(result["blocked"])
        self.assertIn(self.entity_name, result["message"])

    @patch("lib.llm.service.guardrail.OpenAI")
    def test_service_initialization(self, mock_openai):
        """
        ModerationService ã®åˆæœŸåŒ–å‡¦ç†ã‚’æ¤œè¨¼ã™ã‚‹

        ã‚·ãƒŠãƒªã‚ª:
            - ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–æ™‚ã« openai_client ãŒæ­£ã—ãåˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª

        æœŸå¾…çµæœ:
            - ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒ None ã§ãªã„ã“ã¨
            - `openai_client` ãŒã‚»ãƒƒãƒˆã•ã‚Œã¦ã„ã‚‹ã“ã¨

        é‡è¦åº¦:
            ä½ â€” åˆæœŸåŒ–ãƒ­ã‚¸ãƒƒã‚¯ãŒç ´ç¶»ã—ã¦ã„ãªã„ã“ã¨ã‚’ä¿è¨¼
        """
        mock_client = Mock()
        mock_openai.return_value = mock_client

        service = ModerationService()
        self.assertIsNotNone(service)
        self.assertIsNotNone(service.openai_client)


class TestModerationServiceIntegration(TestCase):
    """
    ModerationService ã®çµ±åˆãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹

    æ¦‚è¦:
        - ã“ã®ã‚¯ãƒ©ã‚¹ã§ã¯ã€ModerationService ã®è¤‡æ•°ãƒ¡ã‚½ãƒƒãƒ‰ã‚’é€£æºã•ã›ãŸã€Œçµ±åˆçš„ãªå‹•ä½œç¢ºèªã€ã‚’è¡Œã†ã€‚
        - å®Ÿé‹ç”¨ã«è¿‘ã„ãƒ•ãƒ­ãƒ¼ã§ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ãŒä¸€è²«ã—ã¦æ­£ã—ãè¡Œã‚ã‚Œã‚‹ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹ã€‚

    ç‰¹å¾´:
        - å…¥åŠ›ãŠã‚ˆã³å‡ºåŠ›ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸¡æ–¹ã®å‘¼ã³å‡ºã—ã‚’å«ã‚€ãƒ†ã‚¹ãƒˆ
        - API ãƒ¬ã‚¤ãƒ¤ã‚’å«ã‚€ãŒã€ã™ã¹ã¦ãƒ¢ãƒƒã‚¯ã«ã‚ˆã£ã¦å†ç¾

    æ³¨æ„:
        - OpenAI Moderation API ã®å®Ÿéš›ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯å‘¼ã³å‡ºã•ãªã„ï¼ˆå®Œå…¨ãƒ¢ãƒƒã‚¯ï¼‰
        - å®‰å…¨ãªãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’ç”¨ã„ã¦ã€ã‚·ã‚¹ãƒ†ãƒ ãŒã€Œæ­£å¸¸æ™‚ã«ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ã“ã¨ã€ã‚’ä¿è¨¼ã™ã‚‹

    å‚è€ƒ:
        - OpenAI Moderation API ä»•æ§˜: https://platform.openai.com/docs/guides/moderation
    """

    @patch("lib.llm.service.guardrail.OpenAI")
    def setUp(self, mock_openai):
        # OpenAI APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’ãƒ¢ãƒƒã‚¯åŒ–
        self.mock_client = Mock()
        mock_openai.return_value = self.mock_client

        self.service = ModerationService()
        self.entity_name = "çµ±åˆãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£"

        self.mock_safe_response = create_mock_safe_response()
        self.mock_unsafe_response = create_mock_unsafe_response()

    def test_full_moderation_workflow(self):
        """
        [çµ±åˆæ­£å¸¸ç³»] å…¥åŠ›ãƒ»å‡ºåŠ›ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒé€£æºã—ã¦æ­£å¸¸å‹•ä½œã™ã‚‹ã‹ã‚’æ¤œè¨¼ã™ã‚‹

        ã‚·ãƒŠãƒªã‚ª:
            1. å®‰å…¨ãªãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¯¾ã—ã¦ `check_input_moderation` ã‚’å‘¼ã¶
            2. å®‰å…¨ãªAIå‡ºåŠ›ã«å¯¾ã—ã¦ `check_output_moderation` ã‚’å‘¼ã¶
            3. ä¸¡æ–¹ã®APIå‘¼ã³å‡ºã—ã¯ãƒ¢ãƒƒã‚¯ã§å®‰å…¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™

        æœŸå¾…çµæœ:
            - ä¸¡æ–¹ã®ãƒã‚§ãƒƒã‚¯ã§ `blocked=False` ãŒè¿”ã‚‹
            - ãƒ¢ãƒƒã‚¯APIã®å‘¼ã³å‡ºã—ãŒãã‚Œãã‚Œ1å›ãšã¤è¡Œã‚ã‚Œã‚‹

        é‡è¦åº¦:
            é«˜ â€” å…¥åŠ›ãƒ»å‡ºåŠ›ä¸¡é¢ã§æ­£å¸¸ãªã‚µãƒ¼ãƒ“ã‚¹é€£æºã‚’ä¿è¨¼
        """
        # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½¿ã£ã¦çµ±ä¸€
        self.mock_client.moderations.create.return_value = create_mock_safe_response()

        # å…¥åŠ›ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆ
        input_result = self.service.check_input_moderation(
            "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­", self.entity_name
        )
        self.assertEqual(input_result.signal, GuardRailSignal.GREEN)

        # å‡ºåŠ›ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆ
        output_result = self.service.check_output_moderation(
            "ã¯ã„ã€ã¨ã¦ã‚‚è‰¯ã„å¤©æ°—ã§ã™ã€‚ãŠå‡ºã‹ã‘æ—¥å’Œã§ã™ã­", self.entity_name
        )
        self.assertEqual(output_result.signal, GuardRailSignal.GREEN)

        # APIå‘¼ã³å‡ºã—å›æ•°ã®ç¢ºèª
        self.assertEqual(self.mock_client.moderations.create.call_count, 2)


class TestSemanticGuardService(TestCase):
    """
    SemanticGuardService ã®æ„å‘³å·®åˆ†æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ¤œè¨¼ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã€‚

    ä¿¡å·æ©Ÿãƒ¢ãƒ‡ãƒ«ï¼ˆGREEN, YELLOW, REDï¼‰ã«åŸºã¥ã„ãŸã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã®æŒ™å‹•ã‚’ã€
    ä»¥ä¸‹ã®ã‚·ãƒŠãƒªã‚ªã«æ²¿ã£ã¦ãƒ†ã‚¹ãƒˆã—ã¾ã™ï¼š
    1. GREEN: RAG ã«ãƒ’ãƒƒãƒˆã—ãŸå ´åˆï¼ˆç¤¾å†…ãƒŠãƒ¬ãƒƒã‚¸ã«åŸºã¥ãå®‰å…¨ãªå›ç­”ï¼‰
    2. YELLOW: RAG æœªãƒ’ãƒƒãƒˆã ãŒã€LLM ã®å›ç­”ã«ç¦æ­¢ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œãªã„å ´åˆ
    3. RED: LLM ã®å›ç­”ã«ç¦æ­¢ãƒ¯ãƒ¼ãƒ‰ãŒæ„å‘³çš„ã«å«ã¾ã‚Œã‚‹å ´åˆï¼ˆä¾‹å¤–ç™ºç”Ÿï¼‰
    """

    @patch("lib.llm.service.guardrail.chromadb.PersistentClient")
    @patch("lib.llm.service.guardrail.OpenAIEmbeddingFunction")
    def setUp(self, mock_ef, mock_chroma):
        """
        ãƒ†ã‚¹ãƒˆç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€‚
        ChromaDB ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨åŸ‹ã‚è¾¼ã¿é–¢æ•°ã‚’ãƒ¢ãƒƒã‚¯åŒ–ã—ã€
        ç¦æ­¢ãƒ¯ãƒ¼ãƒ‰ç”¨ã¨ RAG ç”¨ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ‡ã‚Šåˆ†ã‘ã¦è¿”å´ã™ã‚‹ã‚ˆã†ã«è¨­å®šã—ã¾ã™ã€‚
        """
        self.mock_chroma_client = mock_chroma.return_value
        self.mock_ef = mock_ef.return_value

        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ¢ãƒƒã‚¯
        self.mock_forbidden_collection = MagicMock()
        self.mock_rag_collection = MagicMock()

        def side_effect(name, **_):
            if name == "forbidden_words":
                return self.mock_forbidden_collection
            return self.mock_rag_collection

        self.mock_chroma_client.get_or_create_collection.side_effect = side_effect

        self.service = SemanticGuardService(api_key="fake-key")

    def test_evaluate_green_on_rag_hit(self):
        """
        ã‚·ãƒŠãƒªã‚ª: RAG ãƒ’ãƒƒãƒˆï¼ˆğŸŸ¢ GREENï¼‰
        - ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãŒ RAG ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å†…ã®æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ãƒ’ãƒƒãƒˆã—ãŸå ´åˆ
        - ä¸€èˆ¬ LLM ã‚’å‘¼ã³å‡ºã™ã“ã¨ãªãã€GREEN ä¿¡å·ãŒè¿”ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
        """
        # RAGãƒ’ãƒƒãƒˆã™ã‚‹çŠ¶æ³ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        self.mock_rag_collection.query.return_value = {"documents": [["some context"]]}

        result = self.service.evaluate("ã“ã‚“ã«ã¡ã¯")

        self.assertEqual(result.signal, GuardRailSignal.GREEN)
        self.assertEqual(result.reason, "RAG_HIT")

    def test_evaluate_yellow_on_rag_miss(self):
        """
        ã‚·ãƒŠãƒªã‚ª: RAG æœªãƒ’ãƒƒãƒˆã‹ã¤å®‰å…¨ãªå›ç­”ï¼ˆğŸŸ¡ YELLOWï¼‰
        - ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãŒ RAG ã«ãƒ’ãƒƒãƒˆã—ãªã„å ´åˆ
        - ä¸€èˆ¬ LLM ã®å›ç­”ãŒç”Ÿæˆã•ã‚Œã€ãã®å†…å®¹ã«ç¦æ­¢ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œãªã„å ´åˆ
        - YELLOW ä¿¡å·ãŒè¿”ã‚Šã€æ­£å¸¸ã«çµ‚äº†ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
        """
        # RAGãƒ’ãƒƒãƒˆã—ãªã„çŠ¶æ³ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        self.mock_rag_collection.query.return_value = {"documents": [[]]}

        # ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ã‚‚ãƒ‘ã‚¹ã™ã‚‹çŠ¶æ³ï¼ˆè·é›¢ãŒé–¾å€¤ 0.35 ä»¥ä¸Šï¼‰
        self.mock_forbidden_collection.query.return_value = {
            "distances": [[0.8]],  # é–¾å€¤ã‚ˆã‚Šå¤§ãã„ï¼ä¼¼ã¦ã„ãªã„
            "documents": [["ç«¶åˆä»–ç¤¾"]],
        }

        def mock_llm_response(_):
            return "å®‰å…¨ãªå›ç­”ã§ã™"

        result = self.service.evaluate("è³ªå•", llm_response_provider=mock_llm_response)

        self.assertEqual(result.signal, GuardRailSignal.YELLOW)
        self.assertEqual(result.reason, "RAG_MISS")

    def test_evaluate_red_on_forbidden_word(self):
        """
        ã‚·ãƒŠãƒªã‚ª: ç¦æ­¢ãƒ¯ãƒ¼ãƒ‰ã®æ¤œçŸ¥ï¼ˆğŸ”´ REDï¼‰
        - ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãŒ RAG ã«ãƒ’ãƒƒãƒˆã›ãšã€ä¸€èˆ¬ LLM ã®å›ç­”ãŒç”Ÿæˆã•ã‚ŒãŸå ´åˆ
        - ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã«ã€ç¦æ­¢ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹: ä½å·æ€¥ä¾¿ï¼‰ãŒæ„å‘³çš„ã«å«ã¾ã‚Œã‚‹å ´åˆ
        - SemanticGuardException ãŒç™ºç”Ÿã—ã€RED ä¿¡å·ãŒè¿”ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
        """
        # RAGãƒ’ãƒƒãƒˆã—ãªã„çŠ¶æ³
        self.mock_rag_collection.query.return_value = {"documents": [[]]}

        # ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ã§ãƒ’ãƒƒãƒˆã™ã‚‹çŠ¶æ³ï¼ˆè·é›¢ãŒé–¾å€¤ 0.35 æœªæº€ï¼‰
        self.mock_forbidden_collection.query.return_value = {
            "distances": [[0.1]],  # é–¾å€¤ã‚ˆã‚Šå°ã•ã„ï¼æ¥µã‚ã¦è¿‘ã„
            "documents": [["ä½å·æ€¥ä¾¿"]],
        }

        def mock_llm_response(_):
            return "ä½å·æ€¥ä¾¿ã§é€ã‚Šã¾ã™"

        with self.assertRaises(SemanticGuardException) as cm:
            self.service.evaluate("è³ªå•", llm_response_provider=mock_llm_response)

        self.assertEqual(cm.exception.result.signal, GuardRailSignal.RED)
        self.assertEqual(cm.exception.result.reason, "FORBIDDEN_WORD_DETECTED")

    def test_guardrail_blocks_forbidden_word(self):
        """
        ã‚·ãƒŠãƒªã‚ª: ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«çµŒç”±ã§ã®ãƒ¯ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
        - SemanticGuardService ã® create_guardrail ã‚’ä½¿ç”¨ã—ã¦ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«é–¢æ•°ã‚’ä½œæˆ
        - ç¦æ­¢ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã—ãŸå ´åˆã«ã€æœŸå¾…é€šã‚Š blocked: True ãŒè¿”ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
        """
        # ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ã§ãƒ’ãƒƒãƒˆã™ã‚‹çŠ¶æ³
        self.mock_forbidden_collection.query.return_value = {
            "distances": [[0.1]],
            "documents": [["ä½å·æ€¥ä¾¿"]],
        }

        guardrail = self.service.create_guardrail()

        # å¼•æ•°ã¯ (context, agent, text)
        result = guardrail(None, None, "ä½å·æ€¥ä¾¿ã§é…é€ã—ã¾ã™")

        self.assertTrue(result["blocked"])
        self.assertIn("ç¦æ­¢ãƒ¯ãƒ¼ãƒ‰", result["message"])

    def test_guardrail_allows_safe_text(self):
        """
        ã‚·ãƒŠãƒªã‚ª: ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«çµŒç”±ã§ã®å®‰å…¨ãªãƒ†ã‚­ã‚¹ãƒˆè¨±å¯
        - ã‚»ãƒ¼ãƒ•ãªãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã—ãŸå ´åˆã«ã€blocked: False ãŒè¿”ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
        """
        # ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ã§ãƒ’ãƒƒãƒˆã—ãªã„çŠ¶æ³
        self.mock_forbidden_collection.query.return_value = {
            "distances": [[0.8]],
            "documents": [["ç«¶åˆä»–ç¤¾"]],
        }

        guardrail = self.service.create_guardrail()

        result = guardrail(None, None, "å®‰å…¨ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")

        self.assertFalse(result["blocked"])
